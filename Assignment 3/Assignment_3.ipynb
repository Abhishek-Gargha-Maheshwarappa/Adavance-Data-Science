{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python (ADS)",
      "language": "python",
      "name": "ads"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW8EJmKtHoQW",
        "colab_type": "text"
      },
      "source": [
        "#**Multi-class Comparison between Choroidal Neovascularization, Diabetic Macular Edema, Drusen, and Normal** ---------------------------------A cure for blindness\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "![alt text](https://cdn.cnn.com/cnnnext/dam/assets/160325143340-andrew-bastawrous-3-super-169.jpg)\n",
        "\n",
        "\n",
        "## **Abstract** : \n",
        "Retinal optical coherence tomography (OCT) is an imaging technique used to capture high-resolution cross sections of the retinas of living patients. \n",
        "These three diease \n",
        "\n",
        "\n",
        "\n",
        "1. #### **Choroidal neovascularization (CNV)** involves the growth of new blood vessels that originate from the choroid through a break in the Bruch membrane into the sub–retinal pigment epithelium (sub-RPE) or subretinal space. CNV is a major cause of visual loss.\n",
        "2. #### **Diabetic Macular Edema (DME)** is an accumulation of fluid in the macula part of the retina that controls our most detailed vision abilities—due to leaking blood vessels.\n",
        "3. #### **Drusen** are yellow deposits under the retina. Drusen are made up of lipids and proteins. Drusen likely do not cause age-related macular degeneration (AMD). But having drusen increases a person's risk of developing AMD\n",
        "\n",
        "\n",
        "\n",
        "Aim of the notebook was for devloping the deep-learning framework for the screening of patients with common treatable blinding retinal diseases\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fnTGcZlE38X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import neccessary library\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pathlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuWGK3KWE38v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47EQvIfaE-Pf",
        "colab_type": "code",
        "outputId": "094451c8-aa14-4600-8d36-86de2458671e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "# Dowload the required data\n",
        "\n",
        "!wget https://data.mendeley.com/datasets/rscbjbr9sj/2/files/5699a1d8-d1b6-45db-bb92-b61051445347/OCT2017.tar.gz?dl=1 \\\n",
        "    -O /OCT2017"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-05 00:49:37--  https://data.mendeley.com/datasets/rscbjbr9sj/2/files/5699a1d8-d1b6-45db-bb92-b61051445347/OCT2017.tar.gz?dl=1\n",
            "Resolving data.mendeley.com (data.mendeley.com)... 162.159.133.86, 162.159.130.86, 2606:4700:7::a29f:8556, ...\n",
            "Connecting to data.mendeley.com (data.mendeley.com)|162.159.133.86|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://md-datasets-public-files-prod.s3.eu-west-1.amazonaws.com/9cfd5550-a37d-4404-9441-860ee091bc67 [following]\n",
            "--2020-04-05 00:49:37--  https://md-datasets-public-files-prod.s3.eu-west-1.amazonaws.com/9cfd5550-a37d-4404-9441-860ee091bc67\n",
            "Resolving md-datasets-public-files-prod.s3.eu-west-1.amazonaws.com (md-datasets-public-files-prod.s3.eu-west-1.amazonaws.com)... 52.218.98.64\n",
            "Connecting to md-datasets-public-files-prod.s3.eu-west-1.amazonaws.com (md-datasets-public-files-prod.s3.eu-west-1.amazonaws.com)|52.218.98.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5793183169 (5.4G) [application/gzip]\n",
            "Saving to: ‘/OCT2017’\n",
            "\n",
            "/OCT2017            100%[===================>]   5.39G  46.1MB/s    in 2m 3s   \n",
            "\n",
            "2020-04-05 00:51:40 (45.0 MB/s) - ‘/OCT2017’ saved [5793183169/5793183169]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7qI2yVjFETD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract the tar file\n",
        "import tarfile\n",
        "tar = tarfile.open(\"/OCT2017\")\n",
        "tar.extractall()\n",
        "tar.close()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjyUxNKME39H",
        "colab_type": "code",
        "outputId": "8d8c2eb8-2116-4efc-a74f-fba8b31dab7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Organizing paths of data locations\n",
        "main_path = pathlib.Path(r\"OCT2017\")\n",
        "train_path = main_path / 'train'\n",
        "test_path = main_path / 'test'\n",
        "val_path = main_path / 'val'\n",
        "\n",
        "train_path"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('OCT2017/train')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F3vH5fIE39V",
        "colab_type": "code",
        "outputId": "049d4c43-ff4a-4dde-e38a-0092fdbf5fea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Directory with our training horse pictures\n",
        "train_eye_dir = os.path.join( main_path / 'train')\n",
        "\n",
        "# Directory with our validation eye pictures\n",
        "validation_eye_dir = os.path.join(main_path / 'val')\n",
        "\n",
        "# Directory with our test eye pictures\n",
        "test_eye_dir = os.path.join(main_path / 'test')\n",
        "train_eye_dir"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'OCT2017/train'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBCMLlA7E39o",
        "colab_type": "code",
        "outputId": "383c46af-1368-4b45-d59d-a01447ba3f53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import random\n",
        "import pathlib\n",
        "train_image_paths = [str(path) for path in list(train_path.glob('*/*.jpeg'))]\n",
        "random.shuffle(train_image_paths)\n",
        "test_image_paths = [str(path) for path in list(test_path.glob('*/*.jpeg'))]\n",
        "val_image_paths = [str(path) for path in list(val_path.glob('*/*.jpeg'))]\n",
        "\n",
        "\n",
        "print('Number of training images:', len(train_image_paths))\n",
        "print('Number of testing images:', len(test_image_paths))\n",
        "print('Number of validation images:', len(val_image_paths))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training images: 83484\n",
            "Number of testing images: 1000\n",
            "Number of validation images: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruz-vB7DE3-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNXSC6nnE3-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_basic = tf.keras.models.Sequential([\n",
        "    # input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(32, (7,7), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(16, (5,5), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(8, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 200 neuron hidden layer\n",
        "    tf.keras.layers.Dense(1000, activation='relu'),\n",
        "    tf.keras.layers.Dense(200, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHOyLy4xE3-d",
        "colab_type": "code",
        "outputId": "5c7f86e6-34af-41fc-bf93-86e09beb3712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "source": [
        "model_basic.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 144, 144, 32)      4736      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 72, 72, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 68, 68, 16)        12816     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 34, 34, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 8)         1160      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 8)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1000)              2049000   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 200)               200200    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 804       \n",
            "=================================================================\n",
            "Total params: 2,268,716\n",
            "Trainable params: 2,268,716\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnFSo9zjE3-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model_basic.compile(loss='mse',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNl3QJkIE3-9",
        "colab_type": "code",
        "outputId": "dacd1fe4-fbf5-423f-f9e6-e4e2d785d31c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'OCT2017/train/',  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=64,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='categorical')\n",
        "\n",
        "\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        'OCT2017/test',  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='categorical')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 83484 images belonging to 4 classes.\n",
            "Found 1000 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxIZjNy2E3_P",
        "colab_type": "code",
        "outputId": "3064c3f7-4061-4501-f8b9-3fcf9d3d4b0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model_basic.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=len(train_image_paths)//64,  \n",
        "      epochs=30,\n",
        "      verbose=1,\n",
        "      validation_data = test_generator,\n",
        "      validation_steps=len(test_image_paths)//20      \n",
        "      )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1304/1304 [==============================] - 363s 278ms/step - loss: 0.0927 - acc: 0.7376 - val_loss: 0.1056 - val_acc: 0.7020\n",
            "Epoch 2/30\n",
            "1304/1304 [==============================] - 313s 240ms/step - loss: 0.0588 - acc: 0.8402 - val_loss: 0.0455 - val_acc: 0.8770\n",
            "Epoch 3/30\n",
            "1304/1304 [==============================] - 260s 199ms/step - loss: 0.0483 - acc: 0.8726 - val_loss: 0.0312 - val_acc: 0.9200\n",
            "Epoch 4/30\n",
            "1304/1304 [==============================] - 254s 195ms/step - loss: 0.0396 - acc: 0.8970 - val_loss: 0.0269 - val_acc: 0.9290\n",
            "Epoch 5/30\n",
            "1304/1304 [==============================] - 252s 193ms/step - loss: 0.0332 - acc: 0.9155 - val_loss: 0.0167 - val_acc: 0.9590\n",
            "Epoch 6/30\n",
            "1304/1304 [==============================] - 252s 193ms/step - loss: 0.0289 - acc: 0.9272 - val_loss: 0.0450 - val_acc: 0.8840\n",
            "Epoch 7/30\n",
            "1304/1304 [==============================] - 254s 195ms/step - loss: 0.0252 - acc: 0.9373 - val_loss: 0.0097 - val_acc: 0.9760\n",
            "Epoch 8/30\n",
            "1304/1304 [==============================] - 252s 193ms/step - loss: 0.0225 - acc: 0.9444 - val_loss: 0.0249 - val_acc: 0.9400\n",
            "Epoch 9/30\n",
            "1304/1304 [==============================] - 249s 191ms/step - loss: 0.0208 - acc: 0.9493 - val_loss: 0.0125 - val_acc: 0.9710\n",
            "Epoch 10/30\n",
            "1304/1304 [==============================] - 247s 189ms/step - loss: 0.0188 - acc: 0.9545 - val_loss: 0.0327 - val_acc: 0.9190\n",
            "Epoch 11/30\n",
            "1304/1304 [==============================] - 247s 190ms/step - loss: 0.0172 - acc: 0.9589 - val_loss: 0.0220 - val_acc: 0.9440\n",
            "Epoch 12/30\n",
            "1304/1304 [==============================] - 248s 190ms/step - loss: 0.0163 - acc: 0.9615 - val_loss: 0.0158 - val_acc: 0.9590\n",
            "Epoch 13/30\n",
            "1304/1304 [==============================] - 250s 192ms/step - loss: 0.0151 - acc: 0.9642 - val_loss: 0.0093 - val_acc: 0.9780\n",
            "Epoch 14/30\n",
            "1304/1304 [==============================] - 251s 193ms/step - loss: 0.0144 - acc: 0.9659 - val_loss: 0.0075 - val_acc: 0.9820\n",
            "Epoch 15/30\n",
            "1304/1304 [==============================] - 249s 191ms/step - loss: 0.0134 - acc: 0.9687 - val_loss: 0.0135 - val_acc: 0.9720\n",
            "Epoch 16/30\n",
            "1304/1304 [==============================] - 247s 190ms/step - loss: 0.0134 - acc: 0.9690 - val_loss: 0.0113 - val_acc: 0.9770\n",
            "Epoch 17/30\n",
            "1304/1304 [==============================] - 248s 190ms/step - loss: 0.0126 - acc: 0.9712 - val_loss: 0.0138 - val_acc: 0.9690\n",
            "Epoch 18/30\n",
            "1304/1304 [==============================] - 247s 190ms/step - loss: 0.0114 - acc: 0.9738 - val_loss: 0.0086 - val_acc: 0.9820\n",
            "Epoch 19/30\n",
            "1304/1304 [==============================] - 247s 189ms/step - loss: 0.0117 - acc: 0.9734 - val_loss: 0.0151 - val_acc: 0.9660\n",
            "Epoch 20/30\n",
            "1304/1304 [==============================] - 247s 190ms/step - loss: 0.0121 - acc: 0.9725 - val_loss: 0.0153 - val_acc: 0.9630\n",
            "Epoch 21/30\n",
            "1304/1304 [==============================] - 249s 191ms/step - loss: 0.0117 - acc: 0.9734 - val_loss: 0.0194 - val_acc: 0.9580\n",
            "Epoch 22/30\n",
            "1304/1304 [==============================] - 247s 189ms/step - loss: 0.0112 - acc: 0.9747 - val_loss: 0.0086 - val_acc: 0.9820\n",
            "Epoch 23/30\n",
            "1304/1304 [==============================] - 247s 190ms/step - loss: 0.0117 - acc: 0.9739 - val_loss: 0.0145 - val_acc: 0.9690\n",
            "Epoch 24/30\n",
            "1304/1304 [==============================] - 246s 188ms/step - loss: 0.0113 - acc: 0.9752 - val_loss: 0.0070 - val_acc: 0.9850\n",
            "Epoch 25/30\n",
            "1304/1304 [==============================] - 245s 188ms/step - loss: 0.0124 - acc: 0.9728 - val_loss: 0.0157 - val_acc: 0.9640\n",
            "Epoch 26/30\n",
            "1304/1304 [==============================] - 244s 187ms/step - loss: 0.0114 - acc: 0.9751 - val_loss: 0.0143 - val_acc: 0.9690\n",
            "Epoch 27/30\n",
            "1304/1304 [==============================] - 245s 188ms/step - loss: 0.0119 - acc: 0.9740 - val_loss: 0.0136 - val_acc: 0.9720\n",
            "Epoch 28/30\n",
            "1304/1304 [==============================] - 246s 189ms/step - loss: 0.0116 - acc: 0.9748 - val_loss: 0.0145 - val_acc: 0.9680\n",
            "Epoch 29/30\n",
            "1304/1304 [==============================] - 246s 189ms/step - loss: 0.0118 - acc: 0.9745 - val_loss: 0.0175 - val_acc: 0.9640\n",
            "Epoch 30/30\n",
            "1304/1304 [==============================] - 245s 188ms/step - loss: 0.0120 - acc: 0.9742 - val_loss: 0.0093 - val_acc: 0.9810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oEiUz5ENioQ",
        "colab_type": "text"
      },
      "source": [
        "### Trying with different Epoch of 15  with stopping fucntion at 97% accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inK3U82w1IY2",
        "colab_type": "text"
      },
      "source": [
        "###Defining **Callback** for stoping training after 97% accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7ZjfWYP1dBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs={}):\n",
        "            if(logs.get('acc')>0.97):\n",
        "              print(\"\\nReached 97% accuracy so cancelling training!\")\n",
        "              self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH_EcAc-NpAE",
        "colab_type": "code",
        "outputId": "8ee1a245-ef23-4f5d-c36e-05d939887f51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "model_ep = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(32, (7,7), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(16, (5,5), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(8, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 200 neuron hidden layer\n",
        "    tf.keras.layers.Dense(1000, activation='relu'),\n",
        "    tf.keras.layers.Dense(200, activation='relu'),n\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model_ep.compile(loss='mse',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model_ep.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=len(train_image_paths)//64,  \n",
        "      epochs=15,\n",
        "      verbose=1,\n",
        "      validation_data = test_generator,\n",
        "      validation_steps=len(test_image_paths)//20,\n",
        "      callbacks=[callbacks]      \n",
        "      )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1304/1304 [==============================] - 246s 189ms/step - loss: 0.0881 - acc: 0.7520 - val_loss: 0.0647 - val_acc: 0.8270\n",
            "Epoch 2/15\n",
            "1304/1304 [==============================] - 246s 189ms/step - loss: 0.0569 - acc: 0.8465 - val_loss: 0.0415 - val_acc: 0.8900\n",
            "Epoch 3/15\n",
            "1304/1304 [==============================] - 245s 188ms/step - loss: 0.0452 - acc: 0.8813 - val_loss: 0.0232 - val_acc: 0.9370\n",
            "Epoch 4/15\n",
            "1304/1304 [==============================] - 245s 188ms/step - loss: 0.0370 - acc: 0.9043 - val_loss: 0.0194 - val_acc: 0.9550\n",
            "Epoch 5/15\n",
            "1304/1304 [==============================] - 247s 190ms/step - loss: 0.0307 - acc: 0.9226 - val_loss: 0.0231 - val_acc: 0.9390\n",
            "Epoch 6/15\n",
            "1304/1304 [==============================] - 248s 190ms/step - loss: 0.0261 - acc: 0.9350 - val_loss: 0.0178 - val_acc: 0.9530\n",
            "Epoch 7/15\n",
            "1304/1304 [==============================] - 246s 189ms/step - loss: 0.0230 - acc: 0.9429 - val_loss: 0.0174 - val_acc: 0.9600\n",
            "Epoch 8/15\n",
            "1304/1304 [==============================] - 244s 187ms/step - loss: 0.0204 - acc: 0.9501 - val_loss: 0.0132 - val_acc: 0.9700\n",
            "Epoch 9/15\n",
            "1304/1304 [==============================] - 242s 186ms/step - loss: 0.0181 - acc: 0.9562 - val_loss: 0.0133 - val_acc: 0.9700\n",
            "Epoch 10/15\n",
            "1304/1304 [==============================] - 244s 187ms/step - loss: 0.0164 - acc: 0.9600 - val_loss: 0.0082 - val_acc: 0.9810\n",
            "Epoch 11/15\n",
            "1304/1304 [==============================] - 243s 186ms/step - loss: 0.0154 - acc: 0.9629 - val_loss: 0.0117 - val_acc: 0.9740\n",
            "Epoch 12/15\n",
            "1304/1304 [==============================] - 240s 184ms/step - loss: 0.0140 - acc: 0.9668 - val_loss: 0.0079 - val_acc: 0.9820\n",
            "Epoch 13/15\n",
            "1304/1304 [==============================] - 244s 187ms/step - loss: 0.0133 - acc: 0.9687 - val_loss: 0.0205 - val_acc: 0.9520\n",
            "Epoch 14/15\n",
            "1304/1304 [==============================] - ETA: 0s - loss: 0.0125 - acc: 0.9705\n",
            "Reached 97% accuracy so cancelling training!\n",
            "1304/1304 [==============================] - 249s 191ms/step - loss: 0.0125 - acc: 0.9705 - val_loss: 0.0118 - val_acc: 0.9730\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM2XWLQFYxjb",
        "colab_type": "text"
      },
      "source": [
        "Network plateaued with 97% at 15 epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "purg8EFNcnGZ",
        "colab_type": "text"
      },
      "source": [
        "###Using other Activation fucntion - TanH\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEfC68Uic6qK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_act = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(32, (7,7), activation='tanh', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(16, (5,5), activation='tanh'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(8, (3,3), activation='tanh'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(1000, activation='tanh'),\n",
        "    tf.keras.layers.Dense(200, activation='tanh'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi1ohmlfdcV4",
        "colab_type": "code",
        "outputId": "ef06af6b-1d19-4f59-825d-779d6fb1c789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model_act.compile(loss='mse',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model_act.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=len(train_image_paths)//64,  \n",
        "      epochs=15,\n",
        "      verbose=1,\n",
        "      validation_data = test_generator,\n",
        "      validation_steps=len(test_image_paths)//20,\n",
        "      callbacks=[callbacks]      \n",
        "      )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1304/1304 [==============================] - 261s 200ms/step - loss: 0.1792 - acc: 0.4219 - val_loss: 0.2005 - val_acc: 0.2370\n",
            "Epoch 2/15\n",
            "1304/1304 [==============================] - 268s 205ms/step - loss: 0.1479 - acc: 0.5354 - val_loss: 0.1299 - val_acc: 0.6000\n",
            "Epoch 3/15\n",
            "1304/1304 [==============================] - 270s 207ms/step - loss: 0.0774 - acc: 0.7872 - val_loss: 0.0591 - val_acc: 0.8420\n",
            "Epoch 4/15\n",
            "1304/1304 [==============================] - 266s 204ms/step - loss: 0.0589 - acc: 0.8408 - val_loss: 0.0418 - val_acc: 0.8850\n",
            "Epoch 5/15\n",
            "1304/1304 [==============================] - 271s 208ms/step - loss: 0.0492 - acc: 0.8700 - val_loss: 0.0294 - val_acc: 0.9250\n",
            "Epoch 6/15\n",
            "1304/1304 [==============================] - 273s 209ms/step - loss: 0.0407 - acc: 0.8932 - val_loss: 0.0214 - val_acc: 0.9440\n",
            "Epoch 7/15\n",
            "1304/1304 [==============================] - 269s 206ms/step - loss: 0.0346 - acc: 0.9108 - val_loss: 0.0205 - val_acc: 0.9440\n",
            "Epoch 8/15\n",
            "1304/1304 [==============================] - 268s 205ms/step - loss: 0.0292 - acc: 0.9258 - val_loss: 0.0213 - val_acc: 0.9400\n",
            "Epoch 9/15\n",
            "1304/1304 [==============================] - 270s 207ms/step - loss: 0.0256 - acc: 0.9358 - val_loss: 0.0219 - val_acc: 0.9410\n",
            "Epoch 10/15\n",
            "1304/1304 [==============================] - 273s 209ms/step - loss: 0.0228 - acc: 0.9433 - val_loss: 0.0207 - val_acc: 0.9440\n",
            "Epoch 11/15\n",
            "1304/1304 [==============================] - 263s 202ms/step - loss: 0.0201 - acc: 0.9499 - val_loss: 0.0094 - val_acc: 0.9790\n",
            "Epoch 12/15\n",
            "1304/1304 [==============================] - 263s 201ms/step - loss: 0.0184 - acc: 0.9550 - val_loss: 0.0134 - val_acc: 0.9610\n",
            "Epoch 13/15\n",
            "1304/1304 [==============================] - 253s 194ms/step - loss: 0.0166 - acc: 0.9595 - val_loss: 0.0135 - val_acc: 0.9660\n",
            "Epoch 14/15\n",
            "1304/1304 [==============================] - 249s 191ms/step - loss: 0.0151 - acc: 0.9637 - val_loss: 0.0159 - val_acc: 0.9630\n",
            "Epoch 15/15\n",
            "1304/1304 [==============================] - 248s 190ms/step - loss: 0.0143 - acc: 0.9650 - val_loss: 0.0188 - val_acc: 0.9510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8P1rPJYY7U7",
        "colab_type": "text"
      },
      "source": [
        "Using Tanh function reduced the the accuracy to 96.56 from 97%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLJiaxD7qf5X",
        "colab_type": "text"
      },
      "source": [
        "### Other Gradient estimation -  **Stochastic gradient descent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM55FXhFrZqi",
        "colab_type": "code",
        "outputId": "9728645b-88d0-43e2-d21c-482bee5d3585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_gd = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(32, (7,7), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(16, (5,5), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(8, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(1000, activation='relu'),\n",
        "    tf.keras.layers.Dense(200, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model_gd.compile(loss='mse',\n",
        "              optimizer=SGD(lr=0.001),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model_gd.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=len(train_image_paths)//64,  \n",
        "      epochs= 30,\n",
        "      verbose=1,\n",
        "      validation_data = test_generator,\n",
        "      validation_steps=len(test_image_paths)//20,\n",
        "       callbacks=[callbacks]       \n",
        "      )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-20-2ba8d8fc8e13>:32: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/30\n",
            "1304/1304 [==============================] - 342s 262ms/step - loss: 0.1740 - acc: 0.4341 - val_loss: 0.2002 - val_acc: 0.2500\n",
            "Epoch 2/30\n",
            "1304/1304 [==============================] - 294s 226ms/step - loss: 0.1656 - acc: 0.4598 - val_loss: 0.2026 - val_acc: 0.3130\n",
            "Epoch 3/30\n",
            "1304/1304 [==============================] - 261s 200ms/step - loss: 0.1640 - acc: 0.4869 - val_loss: 0.2018 - val_acc: 0.3490\n",
            "Epoch 4/30\n",
            "1304/1304 [==============================] - 257s 197ms/step - loss: 0.1625 - acc: 0.5042 - val_loss: 0.1986 - val_acc: 0.3940\n",
            "Epoch 5/30\n",
            "1304/1304 [==============================] - 257s 197ms/step - loss: 0.1608 - acc: 0.5189 - val_loss: 0.2007 - val_acc: 0.3730\n",
            "Epoch 6/30\n",
            "1304/1304 [==============================] - 255s 196ms/step - loss: 0.1584 - acc: 0.5321 - val_loss: 0.1942 - val_acc: 0.4100\n",
            "Epoch 7/30\n",
            "1304/1304 [==============================] - 253s 194ms/step - loss: 0.1553 - acc: 0.5455 - val_loss: 0.1908 - val_acc: 0.4200\n",
            "Epoch 8/30\n",
            "1304/1304 [==============================] - 251s 193ms/step - loss: 0.1517 - acc: 0.5587 - val_loss: 0.1827 - val_acc: 0.4460\n",
            "Epoch 9/30\n",
            "1304/1304 [==============================] - 252s 193ms/step - loss: 0.1479 - acc: 0.5684 - val_loss: 0.1790 - val_acc: 0.4380\n",
            "Epoch 10/30\n",
            "1304/1304 [==============================] - 251s 193ms/step - loss: 0.1443 - acc: 0.5752 - val_loss: 0.1740 - val_acc: 0.4570\n",
            "Epoch 11/30\n",
            "1304/1304 [==============================] - 250s 192ms/step - loss: 0.1412 - acc: 0.5830 - val_loss: 0.1719 - val_acc: 0.4510\n",
            "Epoch 12/30\n",
            "1304/1304 [==============================] - 252s 194ms/step - loss: 0.1381 - acc: 0.5912 - val_loss: 0.1688 - val_acc: 0.4610\n",
            "Epoch 13/30\n",
            "1304/1304 [==============================] - 256s 196ms/step - loss: 0.1354 - acc: 0.5994 - val_loss: 0.1652 - val_acc: 0.4740\n",
            "Epoch 14/30\n",
            "1304/1304 [==============================] - 255s 195ms/step - loss: 0.1328 - acc: 0.6081 - val_loss: 0.1565 - val_acc: 0.4930\n",
            "Epoch 15/30\n",
            "1304/1304 [==============================] - 253s 194ms/step - loss: 0.1306 - acc: 0.6158 - val_loss: 0.1582 - val_acc: 0.4950\n",
            "Epoch 16/30\n",
            "1304/1304 [==============================] - ETA: 0s - loss: 0.1284 - acc: 0.6241Epoch 17/30\n",
            "1304/1304 [==============================] - 251s 192ms/step - loss: 0.1264 - acc: 0.6306 - val_loss: 0.1485 - val_acc: 0.5370\n",
            "Epoch 18/30\n",
            "1304/1304 [==============================] - 250s 192ms/step - loss: 0.1246 - acc: 0.6367 - val_loss: 0.1410 - val_acc: 0.5610\n",
            "Epoch 19/30\n",
            "1304/1304 [==============================] - 251s 192ms/step - loss: 0.1230 - acc: 0.6416 - val_loss: 0.1492 - val_acc: 0.5280\n",
            "Epoch 20/30\n",
            "1304/1304 [==============================] - 253s 194ms/step - loss: 0.1213 - acc: 0.6476 - val_loss: 0.1485 - val_acc: 0.5240\n",
            "Epoch 21/30\n",
            "1304/1304 [==============================] - 252s 193ms/step - loss: 0.1196 - acc: 0.6528 - val_loss: 0.1424 - val_acc: 0.5550\n",
            "Epoch 22/30\n",
            "1304/1304 [==============================] - 252s 193ms/step - loss: 0.1181 - acc: 0.6581 - val_loss: 0.1421 - val_acc: 0.5600\n",
            "Epoch 23/30\n",
            "1304/1304 [==============================] - 251s 192ms/step - loss: 0.1167 - acc: 0.6614 - val_loss: 0.1398 - val_acc: 0.5560\n",
            "Epoch 24/30\n",
            "1304/1304 [==============================] - 251s 193ms/step - loss: 0.1152 - acc: 0.6665 - val_loss: 0.1374 - val_acc: 0.5690\n",
            "Epoch 25/30\n",
            "1304/1304 [==============================] - 251s 192ms/step - loss: 0.1138 - acc: 0.6717 - val_loss: 0.1563 - val_acc: 0.5130\n",
            "Epoch 26/30\n",
            "1304/1304 [==============================] - 252s 193ms/step - loss: 0.1129 - acc: 0.6753 - val_loss: 0.1337 - val_acc: 0.5710\n",
            "Epoch 27/30\n",
            "1304/1304 [==============================] - 254s 195ms/step - loss: 0.1111 - acc: 0.6789 - val_loss: 0.1291 - val_acc: 0.5910\n",
            "Epoch 28/30\n",
            "1304/1304 [==============================] - 255s 195ms/step - loss: 0.1101 - acc: 0.6827 - val_loss: 0.1331 - val_acc: 0.5700\n",
            "Epoch 29/30\n",
            "1304/1304 [==============================] - 252s 194ms/step - loss: 0.1088 - acc: 0.6858 - val_loss: 0.1314 - val_acc: 0.5920\n",
            "Epoch 30/30\n",
            "1304/1304 [==============================] - 252s 193ms/step - loss: 0.1074 - acc: 0.6913 - val_loss: 0.1191 - val_acc: 0.6100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coVJsulYZHZt",
        "colab_type": "text"
      },
      "source": [
        "Using Stochastic gradient descent, this makes network to take more time to plateau. Even after 30 epcoh it will not converege."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-eYw3FabpVT",
        "colab_type": "text"
      },
      "source": [
        "###Trying with different architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4LCmbrvbwSF",
        "colab_type": "code",
        "outputId": "cc979abf-ed80-4406-c1fa-0c34f97ed6ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "modelt = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(64, (14,14), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(32, (7,7), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(16, (5,5), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(8, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    tf.keras.layers.Dense(200, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "modelt.summary()\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "modelt.compile(loss='mse',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])\n",
        "\n",
        "hit2 = modelt.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=len(train_image_paths)//64,  \n",
        "      epochs=25,\n",
        "      verbose=1,\n",
        "      validation_data = test_generator,\n",
        "      validation_steps=len(test_image_paths)//20,\n",
        "       callbacks=[callbacks]       \n",
        "      )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 137, 137, 64)      37696     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 68, 68, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 62, 62, 32)        100384    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 27, 27, 16)        12816     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 11, 11, 8)         1160      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 5, 5, 8)           0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 200)               40200     \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 4)                 804       \n",
            "=================================================================\n",
            "Total params: 193,060\n",
            "Trainable params: 193,060\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "1304/1304 [==============================] - 306s 235ms/step - loss: 0.0933 - acc: 0.7358 - val_loss: 0.0560 - val_acc: 0.8490\n",
            "Epoch 2/25\n",
            "1304/1304 [==============================] - 306s 234ms/step - loss: 0.0573 - acc: 0.8472 - val_loss: 0.0496 - val_acc: 0.8630\n",
            "Epoch 3/25\n",
            "1304/1304 [==============================] - 308s 236ms/step - loss: 0.0478 - acc: 0.8746 - val_loss: 0.0223 - val_acc: 0.9380\n",
            "Epoch 4/25\n",
            "1304/1304 [==============================] - 310s 238ms/step - loss: 0.0421 - acc: 0.8902 - val_loss: 0.0507 - val_acc: 0.8710\n",
            "Epoch 5/25\n",
            "1304/1304 [==============================] - 314s 241ms/step - loss: 0.0385 - acc: 0.9013 - val_loss: 0.0097 - val_acc: 0.9770\n",
            "Epoch 6/25\n",
            "1304/1304 [==============================] - 310s 238ms/step - loss: 0.0364 - acc: 0.9068 - val_loss: 0.0088 - val_acc: 0.9830\n",
            "Epoch 7/25\n",
            "1304/1304 [==============================] - 309s 237ms/step - loss: 0.0340 - acc: 0.9129 - val_loss: 0.0307 - val_acc: 0.9120\n",
            "Epoch 8/25\n",
            "1304/1304 [==============================] - 308s 236ms/step - loss: 0.0325 - acc: 0.9184 - val_loss: 0.0101 - val_acc: 0.9750\n",
            "Epoch 9/25\n",
            "1304/1304 [==============================] - 301s 231ms/step - loss: 0.0311 - acc: 0.9214 - val_loss: 0.0205 - val_acc: 0.9440\n",
            "Epoch 10/25\n",
            "1304/1304 [==============================] - 303s 233ms/step - loss: 0.0300 - acc: 0.9246 - val_loss: 0.0052 - val_acc: 0.9870\n",
            "Epoch 11/25\n",
            "1304/1304 [==============================] - 300s 230ms/step - loss: 0.0292 - acc: 0.9273 - val_loss: 0.0093 - val_acc: 0.9770\n",
            "Epoch 12/25\n",
            "1304/1304 [==============================] - 304s 233ms/step - loss: 0.0278 - acc: 0.9314 - val_loss: 0.0069 - val_acc: 0.9800\n",
            "Epoch 13/25\n",
            "1304/1304 [==============================] - 306s 235ms/step - loss: 0.0275 - acc: 0.9320 - val_loss: 0.0063 - val_acc: 0.9810\n",
            "Epoch 14/25\n",
            "1304/1304 [==============================] - 300s 230ms/step - loss: 0.0265 - acc: 0.9344 - val_loss: 0.0178 - val_acc: 0.9530\n",
            "Epoch 15/25\n",
            "1304/1304 [==============================] - 299s 230ms/step - loss: 0.0261 - acc: 0.9358 - val_loss: 0.0084 - val_acc: 0.9780\n",
            "Epoch 16/25\n",
            "1304/1304 [==============================] - 300s 230ms/step - loss: 0.0255 - acc: 0.9380 - val_loss: 0.0106 - val_acc: 0.9700\n",
            "Epoch 17/25\n",
            "1304/1304 [==============================] - 291s 223ms/step - loss: 0.0251 - acc: 0.9388 - val_loss: 0.0052 - val_acc: 0.9870\n",
            "Epoch 18/25\n",
            "1304/1304 [==============================] - 290s 222ms/step - loss: 0.0251 - acc: 0.9394 - val_loss: 0.0143 - val_acc: 0.9680\n",
            "Epoch 19/25\n",
            "1304/1304 [==============================] - 288s 221ms/step - loss: 0.0250 - acc: 0.9399 - val_loss: 0.0185 - val_acc: 0.9580\n",
            "Epoch 20/25\n",
            "1304/1304 [==============================] - 288s 221ms/step - loss: 0.0248 - acc: 0.9412 - val_loss: 0.0078 - val_acc: 0.9810\n",
            "Epoch 21/25\n",
            "1304/1304 [==============================] - 289s 221ms/step - loss: 0.0256 - acc: 0.9394 - val_loss: 0.0112 - val_acc: 0.9770\n",
            "Epoch 22/25\n",
            "1304/1304 [==============================] - 291s 223ms/step - loss: 0.0250 - acc: 0.9415 - val_loss: 0.0055 - val_acc: 0.9860\n",
            "Epoch 23/25\n",
            "1304/1304 [==============================] - 290s 222ms/step - loss: 0.0248 - acc: 0.9419 - val_loss: 0.0079 - val_acc: 0.9820\n",
            "Epoch 24/25\n",
            "1304/1304 [==============================] - 288s 221ms/step - loss: 0.0257 - acc: 0.9405 - val_loss: 0.0097 - val_acc: 0.9780\n",
            "Epoch 25/25\n",
            "1304/1304 [==============================] - 288s 221ms/step - loss: 0.0254 - acc: 0.9421 - val_loss: 0.0058 - val_acc: 0.9880\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoTOeakUZdEf",
        "colab_type": "text"
      },
      "source": [
        "Adding one more extra layer lead to decrease in the accuracy to 94% from 97%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMlbrXjmaOGY",
        "colab_type": "text"
      },
      "source": [
        "Changing the weight "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woeh-ZtvaQ6G",
        "colab_type": "code",
        "outputId": "9695d855-cd7a-45c7-e389-e8dabc1780a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        }
      },
      "source": [
        "tf.keras.initializers.TruncatedNormal(\n",
        "    mean=0.0, stddev=0.05, seed=None\n",
        ")\n",
        "\n",
        "model_int = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(32, (7,7), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(16, (5,5), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(8, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(1000, activation='relu'),\n",
        "    tf.keras.layers.Dense(200, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model_int.compile(loss='mse',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model_int.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=len(train_image_paths)//64,  \n",
        "      epochs=30,\n",
        "      verbose=1,\n",
        "      validation_data = test_generator,\n",
        "      validation_steps=len(test_image_paths)//20,\n",
        "             callbacks=[callbacks]       \n",
        "     \n",
        "      )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1304/1304 [==============================] - ETA: 0s - loss: 0.0905 - acc: 0.7432Epoch 2/30\n",
            "1304/1304 [==============================] - 303s 232ms/step - loss: 0.0552 - acc: 0.8523 - val_loss: 0.0445 - val_acc: 0.8830\n",
            "Epoch 3/30\n",
            "1304/1304 [==============================] - 258s 198ms/step - loss: 0.0429 - acc: 0.8884 - val_loss: 0.0314 - val_acc: 0.9110\n",
            "Epoch 4/30\n",
            "1304/1304 [==============================] - 253s 194ms/step - loss: 0.0350 - acc: 0.9102 - val_loss: 0.0193 - val_acc: 0.9490\n",
            "Epoch 5/30\n",
            "1304/1304 [==============================] - 252s 193ms/step - loss: 0.0292 - acc: 0.9263 - val_loss: 0.0138 - val_acc: 0.9660\n",
            "Epoch 6/30\n",
            "1304/1304 [==============================] - 251s 193ms/step - loss: 0.0252 - acc: 0.9378 - val_loss: 0.0176 - val_acc: 0.9560\n",
            "Epoch 7/30\n",
            "1304/1304 [==============================] - 251s 193ms/step - loss: 0.0222 - acc: 0.9458 - val_loss: 0.0157 - val_acc: 0.9620\n",
            "Epoch 8/30\n",
            "1304/1304 [==============================] - 251s 192ms/step - loss: 0.0194 - acc: 0.9533 - val_loss: 0.0096 - val_acc: 0.9780\n",
            "Epoch 9/30\n",
            "1304/1304 [==============================] - 254s 195ms/step - loss: 0.0177 - acc: 0.9578 - val_loss: 0.0217 - val_acc: 0.9520\n",
            "Epoch 10/30\n",
            "1304/1304 [==============================] - 251s 192ms/step - loss: 0.0162 - acc: 0.9619 - val_loss: 0.0216 - val_acc: 0.9530\n",
            "Epoch 11/30\n",
            "1304/1304 [==============================] - 250s 192ms/step - loss: 0.0152 - acc: 0.9640 - val_loss: 0.0086 - val_acc: 0.9790\n",
            "Epoch 12/30\n",
            "1304/1304 [==============================] - 266s 204ms/step - loss: 0.0140 - acc: 0.9674 - val_loss: 0.0112 - val_acc: 0.9750\n",
            "Epoch 13/30\n",
            "1304/1304 [==============================] - 263s 202ms/step - loss: 0.0133 - acc: 0.9691 - val_loss: 0.0106 - val_acc: 0.9760\n",
            "Epoch 14/30\n",
            "1304/1304 [==============================] - ETA: 0s - loss: 0.0129 - acc: 0.9705\n",
            "Reached 97% accuracy so cancelling training!\n",
            "1304/1304 [==============================] - 260s 200ms/step - loss: 0.0129 - acc: 0.9705 - val_loss: 0.0094 - val_acc: 0.9780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zwGAy6kZpCO",
        "colab_type": "text"
      },
      "source": [
        "Changing the the intial weight not much change with out accuracy but network convereged sooner than the default weight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkCyBRt8IMZh",
        "colab_type": "text"
      },
      "source": [
        "Trying with different Cost function MAE Mean Absolute Error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3buGociGILjB",
        "colab_type": "code",
        "outputId": "8d4a0c9f-4fc1-4e07-9008-18218fed79db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_int = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(32, (7,7), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(16, (5,5), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(8, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(1000, activation='relu'),\n",
        "    tf.keras.layers.Dense(200, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model_int.compile(loss='mae',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model_int.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=len(train_image_paths)//64,  \n",
        "      epochs=30,\n",
        "      verbose=1,\n",
        "      validation_data = test_generator,\n",
        "      validation_steps=len(test_image_paths)//20,\n",
        "             callbacks=[callbacks]       \n",
        "     \n",
        "      )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-16-fcc1db8ee541>:33: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/30\n",
            "1304/1304 [==============================] - 374s 287ms/step - loss: 0.2772 - acc: 0.4454 - val_loss: 0.3750 - val_acc: 0.2500\n",
            "Epoch 2/30\n",
            "1304/1304 [==============================] - 326s 250ms/step - loss: 0.2772 - acc: 0.4456 - val_loss: 0.3750 - val_acc: 0.2500\n",
            "Epoch 3/30\n",
            "1304/1304 [==============================] - 281s 215ms/step - loss: 0.2772 - acc: 0.4457 - val_loss: 0.3715 - val_acc: 0.2570\n",
            "Epoch 4/30\n",
            "1304/1304 [==============================] - 282s 216ms/step - loss: 0.2771 - acc: 0.4457 - val_loss: 0.3780 - val_acc: 0.2440\n",
            "Epoch 5/30\n",
            "1304/1304 [==============================] - 266s 204ms/step - loss: 0.2772 - acc: 0.4456 - val_loss: 0.3735 - val_acc: 0.2530\n",
            "Epoch 6/30\n",
            "1304/1304 [==============================] - 264s 203ms/step - loss: 0.2771 - acc: 0.4459 - val_loss: 0.3770 - val_acc: 0.2460\n",
            "Epoch 7/30\n",
            "1304/1304 [==============================] - 262s 201ms/step - loss: 0.2773 - acc: 0.4455 - val_loss: 0.3755 - val_acc: 0.2490\n",
            "Epoch 8/30\n",
            "1304/1304 [==============================] - 263s 201ms/step - loss: 0.2772 - acc: 0.4455 - val_loss: 0.3710 - val_acc: 0.2580\n",
            "Epoch 9/30\n",
            "1304/1304 [==============================] - 263s 202ms/step - loss: 0.2772 - acc: 0.4456 - val_loss: 0.3815 - val_acc: 0.2370\n",
            "Epoch 10/30\n",
            "1304/1304 [==============================] - 261s 200ms/step - loss: 0.2770 - acc: 0.4460 - val_loss: 0.3740 - val_acc: 0.2520\n",
            "Epoch 11/30\n",
            "1304/1304 [==============================] - 261s 201ms/step - loss: 0.2774 - acc: 0.4453 - val_loss: 0.3765 - val_acc: 0.2470\n",
            "Epoch 12/30\n",
            "1304/1304 [==============================] - 261s 200ms/step - loss: 0.2772 - acc: 0.4456 - val_loss: 0.3760 - val_acc: 0.2480\n",
            "Epoch 13/30\n",
            "1304/1304 [==============================] - 262s 201ms/step - loss: 0.2771 - acc: 0.4458 - val_loss: 0.3700 - val_acc: 0.2600\n",
            "Epoch 14/30\n",
            "1304/1304 [==============================] - 264s 203ms/step - loss: 0.2772 - acc: 0.4456 - val_loss: 0.3710 - val_acc: 0.2580\n",
            "Epoch 15/30\n",
            "1304/1304 [==============================] - 263s 201ms/step - loss: 0.2770 - acc: 0.4459 - val_loss: 0.3790 - val_acc: 0.2420\n",
            "Epoch 16/30\n",
            "1304/1304 [==============================] - 263s 202ms/step - loss: 0.2773 - acc: 0.4453 - val_loss: 0.3760 - val_acc: 0.2480\n",
            "Epoch 17/30\n",
            "1304/1304 [==============================] - 264s 202ms/step - loss: 0.2772 - acc: 0.4457 - val_loss: 0.3850 - val_acc: 0.2300\n",
            "Epoch 18/30\n",
            "1304/1304 [==============================] - 262s 201ms/step - loss: 0.2773 - acc: 0.4454 - val_loss: 0.3720 - val_acc: 0.2560\n",
            "Epoch 19/30\n",
            "1304/1304 [==============================] - 262s 201ms/step - loss: 0.2771 - acc: 0.4458 - val_loss: 0.3720 - val_acc: 0.2560\n",
            "Epoch 20/30\n",
            "1304/1304 [==============================] - 263s 202ms/step - loss: 0.2770 - acc: 0.4459 - val_loss: 0.3780 - val_acc: 0.2440\n",
            "Epoch 21/30\n",
            "1304/1304 [==============================] - 263s 202ms/step - loss: 0.2773 - acc: 0.4454 - val_loss: 0.3780 - val_acc: 0.2440\n",
            "Epoch 22/30\n",
            "1304/1304 [==============================] - 262s 201ms/step - loss: 0.2772 - acc: 0.4456 - val_loss: 0.3755 - val_acc: 0.2490\n",
            "Epoch 23/30\n",
            "1304/1304 [==============================] - 262s 201ms/step - loss: 0.2770 - acc: 0.4460 - val_loss: 0.3675 - val_acc: 0.2650\n",
            "Epoch 24/30\n",
            "1304/1304 [==============================] - 263s 202ms/step - loss: 0.2774 - acc: 0.4453 - val_loss: 0.3720 - val_acc: 0.2560\n",
            "Epoch 25/30\n",
            "1304/1304 [==============================] - 262s 201ms/step - loss: 0.2772 - acc: 0.4457 - val_loss: 0.3770 - val_acc: 0.2460\n",
            "Epoch 26/30\n",
            "1304/1304 [==============================] - 261s 200ms/step - loss: 0.2772 - acc: 0.4457 - val_loss: 0.3805 - val_acc: 0.2390\n",
            "Epoch 27/30\n",
            "1304/1304 [==============================] - 261s 200ms/step - loss: 0.2772 - acc: 0.4456 - val_loss: 0.3780 - val_acc: 0.2440\n",
            "Epoch 28/30\n",
            "1304/1304 [==============================] - 263s 202ms/step - loss: 0.2771 - acc: 0.4458 - val_loss: 0.3710 - val_acc: 0.2580\n",
            "Epoch 29/30\n",
            "1304/1304 [==============================] - 261s 200ms/step - loss: 0.2772 - acc: 0.4456 - val_loss: 0.3705 - val_acc: 0.2590\n",
            "Epoch 30/30\n",
            "1304/1304 [==============================] - 262s 201ms/step - loss: 0.2771 - acc: 0.4458 - val_loss: 0.3815 - val_acc: 0.2370\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIn4MgyKZ7BW",
        "colab_type": "text"
      },
      "source": [
        "Using Mean Absolute Error led to accuracy of 44% even after 30 epoch, means the network is not convere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACrPswCbNzHi",
        "colab_type": "text"
      },
      "source": [
        "###**Conclusion**\n",
        "\n",
        "![alt text](data:image/jpeg;base64,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)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZntZ6H9d5MtT",
        "colab_type": "text"
      },
      "source": [
        "1.  With the Deep Learning model of CNN with three layers and dense layer, it was possible to train the model with accuracy of 97%.\n",
        "\n",
        "2. Tried with different number of Epoch of 15 to see how network plateau and the accuracy is lesser than 30 Epoch.\n",
        "\n",
        "3. Activation fucntion - relu and TanH both work fine with first giving 97% accuracy and second being 96 % accuracy.\n",
        "\n",
        "4. Gradient estimation used are Stochastic gradient descent and RmsProp optimizer and network will plateau with accuracy of 97% for second and first SDG even after 30 epoch it has accuracy 67%\n",
        "\n",
        "5. Tried with another Network Architecture with one extra layer got a output accuracy of 94% compared to other with one less layer with 97%\n",
        "\n",
        "6. Tested with two different Network initialization one with TruncatedNormal and other being normal which default both network plateaued at 97%\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URqqST0QOPNI",
        "colab_type": "text"
      },
      "source": [
        "## **Reference**\n",
        "\n",
        "\n",
        "\n",
        "1.   Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning - https://www.cell.com/cell/fulltext/S0092-8674(18)30154-5\n",
        "2.   https://www.tensorflow.org/\n",
        "3.   Dataset - Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification [link text](https://data.mendeley.com/datasets/rscbjbr9sj/2?__hstc=25856994.c1accec21e34f9e5905f815b5f26b6cf.1585211719596.1585211719596.1585297308307.2&__hssc=25856994.2.1585297308307&__hsfp=3321820689)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsdJw6uINcC3",
        "colab_type": "text"
      },
      "source": [
        "Copyright 2020 Abhishek Gargha Maheshwarappa\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    }
  ]
}