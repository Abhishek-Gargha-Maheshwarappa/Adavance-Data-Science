{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning_OpenAI Taxi-v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9-cWDxsXDvO",
        "colab_type": "text"
      },
      "source": [
        "# **Assignment 2 – Reinforcement Learning**\n",
        "\n",
        "**Professor**: Nik Bear Brown\n",
        "\n",
        "**Author**   : Abhishek Gargha Maheshwarappa\n",
        "\n",
        "**Nuid**   : 001375462"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vLFtl7KVUsE",
        "colab_type": "text"
      },
      "source": [
        "**# Abstarct**\n",
        "\n",
        "Designing a simulation of a self-driving cab. The major goal is to demonstrate, in a simplified environment, how you can use RL techniques to develop an efficient smart cab. The aim of the notebook is to Reinforcement Learning and Q Learning to buils cab to \n",
        "\n",
        "\n",
        "1.   Drop off the passenger to the right location.\n",
        "2.   Save passenger's time by taking minimum time possible to drop off\n",
        "3.   Take care of passenger's safety and traffic rules\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyKAsb4zVJzr",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://desktopwalls.net/wp-content/uploads/2014/06/New%20York%20City%20Yellow%20Taxi%20Cab%20Free%20Wallpaper%20HD.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHehDufEXyuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing reguired libraries\n",
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RjA3KUkX3X-",
        "colab_type": "code",
        "outputId": "0c110279-4606-4ffe-8c76-098530ea11fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "#setting enivornment from gym and visulaising the environment\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "env.render()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p73ThuDa5Cg3",
        "colab_type": "code",
        "outputId": "0be23c42-5461-4f03-bd62-54432e207d81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "env.metadata"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'render.modes': ['human', 'ansi']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmoL46WlZVRa",
        "colab_type": "text"
      },
      "source": [
        "## **Actions and State Space**\n",
        "\n",
        "### **Six possible actions:**\n",
        "\n",
        "1. south\n",
        "2. north\n",
        "3. east\n",
        "4. west\n",
        "5. pickup\n",
        "6. dropoff\n",
        "\n",
        "![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRvhjG4jIweDeqUCbWmKMFj3qY2Zg64ZPPAirQrJp265NqFhcrE)\n",
        "\n",
        "###  **State Space**\n",
        "\n",
        "The area can be breaken down into a 5x5 grid, which gives us 25 possible taxi locations. There are four locations that we can pick up and drop off a passenger: \n",
        "\n",
        "R, G, Y, B or [(0,0), (0,4), (4,0), (4,3)] in (row, col) coordinates.\n",
        "\n",
        "One additional passenger state of being inside the taxi, there's four destinations and five passenger locations.\n",
        "\n",
        "So the taxi environment has 5×5×5×4=500 total possible states.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BYJq4L0YFcp",
        "colab_type": "code",
        "outputId": "b40ecf7a-351c-4b36-bfe1-c762d0e1fd3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "action_size = env.action_space.n\n",
        "print(\"Action size \", action_size)\n",
        "\n",
        "state_size = env.observation_space.n\n",
        "print(\"State size \", state_size)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action size  6\n",
            "State size  500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYU5p4r3bV_L",
        "colab_type": "text"
      },
      "source": [
        "* The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
        "* The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
        "* R, G, Y, B are the possible pickup and destination locations. \n",
        "* The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNA8l_i9cUf2",
        "colab_type": "text"
      },
      "source": [
        "## **Rewards**\n",
        "\n",
        "![alt text](https://www.creditcards.com/credit-card-news/wp-content/uploads/track-rewards-like-pro-lg.jpg)\n",
        "\n",
        "* The agent should receive a high positive reward for a successful dropoff because this behavior is highly desired\n",
        "* The agent should be penalized if it tries to drop off a passenger in wrong locations\n",
        "* The agent should get a slight negative reward for not making it to the destination after every time-step. \"Slight\" negative because we would prefer our agent to reach late instead of making wrong moves trying to reach to the destination as fast as possible\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttEwXGt0Ye_Q",
        "colab_type": "code",
        "outputId": "dc777e8b-ecee-4532-b698-73ac711327ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph8QHCpOYjRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "total_episodes = 50000        # Total episodes\n",
        "total_test_episodes = 100     # Total test episodes\n",
        "max_steps = 99                # Max steps per episode\n",
        "\n",
        "learning_rate = 0.7           # Learning rate\n",
        "gamma = 0.618                 # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.01 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgS_5nXuYlkq",
        "colab_type": "code",
        "outputId": "63560ccc-ee1d-426c-98ed-bb39c308eb81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "total_num_step = 0\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0,1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "        \n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        \n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
        "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        total_rewards += reward\n",
        "                \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "    \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    rewards.append(total_rewards)\n",
        "    total_num_step  += step\n",
        "\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: 6.30918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvVt_Hk6Zqzq",
        "colab_type": "text"
      },
      "source": [
        " ### Average number of steps taken per episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cybwI7GuZpXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a9d2dee-209d-427a-96df-290291291ebd"
      },
      "source": [
        "avg = total_num_step/total_episodes\n",
        "\n",
        "print('Average number of steps taken per episode   ', avg)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average number of steps taken per episode    12.96932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGCkQ0kUvCY4",
        "colab_type": "text"
      },
      "source": [
        " Decay rate and starting epsilon were choosen as given in the baseline model later they were tuned for the improvement of the score time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db65BUc4yK-o",
        "colab_type": "text"
      },
      "source": [
        "### Changing the policy to **argmin**\n",
        "\n",
        "Trying with different policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84ejCPxhyKLM",
        "colab_type": "code",
        "outputId": "65b0d10f-a743-4fb6-ba96-b58de003340d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0,1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmin(qtable[state,:])\n",
        "        \n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        \n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
        "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        total_rewards += reward\n",
        "                \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "    \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: -983.9178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqRu7S_CyiVD",
        "colab_type": "text"
      },
      "source": [
        "Trying with second Large value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aj11g_GGOSZ",
        "colab_type": "code",
        "outputId": "2bda675a-2404-4512-eef6-67f3ab2d597a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0,1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argsort((qtable[state,:]))[-2]\n",
        "        \n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        \n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
        "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        total_rewards += reward\n",
        "                \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "    \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: -101.9294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apA3BhMscRE7",
        "colab_type": "text"
      },
      "source": [
        "### Trying with Random chocie based on proablity "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5LRzmiPSvSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0,1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "          from scipy.stats import norm\n",
        "          from numpy.random import choice\n",
        "          x=np.sum(abs(qtable[state,:]))\n",
        "          y=(qtable[state,:]/x)\n",
        "          draw =( choice(qtable[state,:], 1,p=abs(y)))\n",
        "          s = np.where(qtable[state,:] == draw)\n",
        "          s = np.asarray(s)\n",
        "          s.flatten() \n",
        "          import pandas as pd\n",
        "          k = pd.DataFrame(s)\n",
        "          action = k[0][0]\n",
        "        \n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        \n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
        "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        total_rewards += reward\n",
        "                \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "    \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfhUygCpGV0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "456b65ae-3a7f-4fad-8373-6e7b2852b99b"
      },
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        remaing_step =max_steps - step\n",
        "\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0,1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "          if((remaing_step*0.95)< step):\n",
        "            action = np.argsort((qtable[state,:]))[-2]\n",
        "            current_step = step\n",
        "            remaing_step =max_steps - current_step\n",
        "          else:\n",
        "            action = np.argmax((qtable[state,:]))\n",
        "            current_step = step\n",
        "            remaing_step =max_steps - current_step\n",
        "\n",
        "        \n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "            current_step = step\n",
        "            remaing_step =max_steps - current_step\n",
        "        \n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
        "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        total_rewards += reward\n",
        "                \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "    \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: 7.41606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyDUxs-OjSeS",
        "colab_type": "text"
      },
      "source": [
        "Done with Changing policy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reaiXZvMklGc",
        "colab_type": "code",
        "outputId": "29cf71b0-879c-4537-a955-15f90ef63940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "env.reset()\n",
        "rewards = []\n",
        "frames = [] # for animation\n",
        "epochs = 0\n",
        "\n",
        "for episode in range(total_test_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    #print(\"****************************************************\")\n",
        "    #print(\"EPISODE \", episode)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # UNCOMMENT IT IF YOU WANT TO SEE OUR AGENT PLAYING\n",
        "        # env.render()\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        total_rewards += reward\n",
        "        frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "        epochs += 1\n",
        "\n",
        "        if done:\n",
        "            rewards.append(total_rewards)\n",
        "            #print (\"Score\", total_rewards)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: 8.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol7K9FEts-yI",
        "colab_type": "text"
      },
      "source": [
        "## **Run this only to see the gaming being played** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YLagm4pmsZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.5)\n",
        "        \n",
        "print_frames(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWVnNu1Wu2sU",
        "colab_type": "text"
      },
      "source": [
        "This to check the game with 85 G as my destination - Home\n",
        "\n",
        "![alt text](data:image/jpeg;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUywmlPz_ri_",
        "colab_type": "code",
        "outputId": "9f631f9d-ed78-4693-dd77-b1c292f309d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "env.reset()\n",
        "\n",
        "for episode in range(10):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        \n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        if done:\n",
        "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "            env.render()\n",
        "            print(\"state=\",new_state)\n",
        "            if new_state == 85:\n",
        "                print(\"Hey reached my home 🏆\")\n",
        "            else:\n",
        "                print(\"Oh this not my home ☠️\")\n",
        "            \n",
        "            # We print the number of step it took.\n",
        "            print(\"Number of steps\", step)\n",
        "            \n",
        "            break\n",
        "        state = new_state\n",
        "env.close()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "state= 0\n",
            "Oh this not my home ☠️\n",
            "Number of steps 16\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "state= 85\n",
            "Hey reached my home 🏆\n",
            "Number of steps 16\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "state= 85\n",
            "Hey reached my home 🏆\n",
            "Number of steps 16\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "state= 85\n",
            "Hey reached my home 🏆\n",
            "Number of steps 13\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "state= 0\n",
            "Oh this not my home ☠️\n",
            "Number of steps 14\n",
            "****************************************************\n",
            "EPISODE  5\n",
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "state= 0\n",
            "Oh this not my home ☠️\n",
            "Number of steps 16\n",
            "****************************************************\n",
            "EPISODE  6\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "state= 410\n",
            "Oh this not my home ☠️\n",
            "Number of steps 12\n",
            "****************************************************\n",
            "EPISODE  7\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "state= 410\n",
            "Oh this not my home ☠️\n",
            "Number of steps 12\n",
            "****************************************************\n",
            "EPISODE  8\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "state= 85\n",
            "Hey reached my home 🏆\n",
            "Number of steps 14\n",
            "****************************************************\n",
            "EPISODE  9\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "state= 410\n",
            "Oh this not my home ☠️\n",
            "Number of steps 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpK4MBgbxTSG",
        "colab_type": "text"
      },
      "source": [
        "## **Evaluating the agent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6-4lIJ4GXj-",
        "colab_type": "text"
      },
      "source": [
        " Average number of steps taken per episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYXUwlMgxLpw",
        "colab_type": "code",
        "outputId": "62ede7e0-19b8-4071-9ab2-5ce66551ff49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(qtable[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.99\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34sWqepLX8BF",
        "colab_type": "code",
        "outputId": "eab43df7-00be-42e0-bb92-d4fa20b97a50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "\n",
        "\n",
        "total_episodes = 50000        # Total episodes\n",
        "total_test_episodes = 100     # Total test episodes\n",
        "max_steps = 99                # Max steps per episode\n",
        "\n",
        "learning_rate = 0.7           # Learning rate\n",
        "gamma = 0.618                 # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.01             # Exponential decay rate for exploration prob\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjsBY9YlXziP",
        "colab_type": "code",
        "outputId": "766c744c-024d-420e-e48b-c0eb13350db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0,1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "        \n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        \n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
        "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        total_rewards += reward\n",
        "                \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "    \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "print(np.argmax(qtable))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: 6.37714\n",
            "101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybDQVSWsYBXT",
        "colab_type": "text"
      },
      "source": [
        "Score over time: 6.37714\n",
        "\n",
        "This is the baseline model.\n",
        "\n",
        "Now fine tuning the hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWY9YTcrCT8",
        "colab_type": "code",
        "outputId": "0418b31d-b581-45c0-cce7-d00a275c8db2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1bVUnh-uOhk",
        "colab_type": "code",
        "outputId": "61ee0f61-8df8-4e4d-b27b-3f000e1c5e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "\n",
        "l_rate = [0.7,0.1, 0.25, 0.5]\n",
        "\n",
        "\n",
        "for rate in l_rate:\n",
        "  rewards = []\n",
        "  qtable = np.zeros((state_size, action_size))\n",
        "  total_episodes = 50000        # Total episodes\n",
        "  total_test_episodes = 100     # Total test episodes\n",
        "  max_steps = 99                # Max steps per episode\n",
        "\n",
        "  #learning_rate = 0.7           # Learning rate\n",
        "  gamma = 0.618                 # Discounting rate\n",
        "\n",
        "  # Exploration parameters\n",
        "  epsilon = 1.0                 # Exploration rate\n",
        "  max_epsilon = 1.0             # Exploration probability at start\n",
        "  min_epsilon = 0.01            # Minimum exploration probability \n",
        "  decay_rate = 0.01  \n",
        "  learning_rate = rate\n",
        "    # 2 For life or until learning is stopped\n",
        "  for episode in range(total_episodes):\n",
        "      # Reset the environment\n",
        "      state = env.reset()\n",
        "      step = 0\n",
        "      done = False\n",
        "      total_rewards = 0\n",
        "    \n",
        "      for step in range(max_steps):\n",
        "          # 3. Choose an action a in the current world state (s)\n",
        "          ## First we randomize a number\n",
        "          exp_exp_tradeoff = random.uniform(0,1)\n",
        "        \n",
        "          ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "          if exp_exp_tradeoff > epsilon:\n",
        "              action = np.argmax(qtable[state,:])\n",
        "        \n",
        "          # Else doing a random choice --> exploration\n",
        "          else:\n",
        "              action = env.action_space.sample()\n",
        "        \n",
        "          # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "          new_state, reward, done, info = env.step(action)\n",
        "\n",
        "          # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "          qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma *np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "          total_rewards += reward\n",
        "                \n",
        "          # Our new state is state\n",
        "          state = new_state\n",
        "        \n",
        "          # If done : finish episode\n",
        "          if done == True: \n",
        "              break\n",
        "    \n",
        "      # Reduce epsilon (because we need less and less exploration)\n",
        "      epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "      rewards.append(total_rewards)\n",
        "  print('Learning Rate - ',rate)\n",
        "  print (\"Score over time: \" +  str(sum(rewards)/total_episodes))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning Rate -  0.7\n",
            "Score over time: 6.35982\n",
            "Learning Rate -  0.1\n",
            "Score over time: 4.18334\n",
            "Learning Rate -  0.25\n",
            "Score over time: 5.72168\n",
            "Learning Rate -  0.5\n",
            "Score over time: 6.2107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVsbpIfIsdnN",
        "colab_type": "text"
      },
      "source": [
        "**Tunning Alpha ,discount  and decay rate all together.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0zx4KSH0Utc",
        "colab_type": "code",
        "outputId": "1eeb5815-9442-4844-e538-de6e6f649af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "alpha = [0.25,0.5, 0.7, 0.8] # Learning rate\n",
        "Discount_rate = [0.4, 0.618, 0.7]\n",
        "decay = [0.01, 0.10,0.3,0.6,0.8]\n",
        "data =[]\n",
        "\n",
        "for d in decay:\n",
        "  decay_rate = d\n",
        "  for g in Discount_rate:\n",
        "    gamma = g                  # Discounting rate\n",
        "    for rate in alpha:\n",
        "      rewards = []\n",
        "      qtable = np.zeros((state_size, action_size))\n",
        "      total_episodes = 50000        # Total episodes\n",
        "      total_test_episodes = 100     # Total test episodes\n",
        "      max_steps = 99                # Max steps per episode\n",
        "\n",
        "                \n",
        "      #gamma = 0.618                 \n",
        "\n",
        "      # Exploration parameters\n",
        "      epsilon = 1.0                 # Exploration rate\n",
        "      max_epsilon = 1.0             # Exploration probability at start\n",
        "      min_epsilon = 0.01            # Minimum exploration probability \n",
        "      #decay_rate = 0.01  \n",
        "      learning_rate = rate\n",
        "        # 2 For life or until learning is stopped\n",
        "      for episode in range(total_episodes):\n",
        "          # Reset the environment\n",
        "          state = env.reset()\n",
        "          step = 0\n",
        "          done = False\n",
        "          total_rewards = 0\n",
        "        \n",
        "          for step in range(max_steps):\n",
        "              # 3. Choose an action a in the current world state (s)\n",
        "              ## First we randomize a number\n",
        "              exp_exp_tradeoff = random.uniform(0,1)\n",
        "            \n",
        "              ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "              if exp_exp_tradeoff > epsilon:\n",
        "                  action = np.argmax(qtable[state,:])\n",
        "            \n",
        "              # Else doing a random choice --> exploration\n",
        "              else:\n",
        "                  action = env.action_space.sample()\n",
        "            \n",
        "              # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "              new_state, reward, done, info = env.step(action)\n",
        "\n",
        "              # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "              qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma *np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "              total_rewards += reward\n",
        "                    \n",
        "              # Our new state is state\n",
        "              state = new_state\n",
        "            \n",
        "              # If done : finish episode\n",
        "              if done == True: \n",
        "                  break\n",
        "        \n",
        "          # Reduce epsilon (because we need less and less exploration)\n",
        "          epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "          rewards.append(total_rewards)\n",
        "          score=sum(rewards)/total_episodes\n",
        "      data.append([learning_rate,gamma,decay_rate,score])\n",
        "      print('Learning Rate - ',rate)\n",
        "      print('Gamma ', gamma)\n",
        "      print('decay_rate',decay_rate)\n",
        "      print (\"Score over time: \" +  str(sum(rewards)/total_episodes))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning Rate -  0.25\n",
            "Gamma  0.4\n",
            "decay_rate 0.01\n",
            "Score over time: 5.33492\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.4\n",
            "decay_rate 0.01\n",
            "Score over time: 6.04594\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.4\n",
            "decay_rate 0.01\n",
            "Score over time: 6.27336\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.4\n",
            "decay_rate 0.01\n",
            "Score over time: 6.3451\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.618\n",
            "decay_rate 0.01\n",
            "Score over time: 5.71432\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.618\n",
            "decay_rate 0.01\n",
            "Score over time: 6.2397\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.618\n",
            "decay_rate 0.01\n",
            "Score over time: 6.35056\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.618\n",
            "decay_rate 0.01\n",
            "Score over time: 6.43066\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.7\n",
            "decay_rate 0.01\n",
            "Score over time: 5.78414\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.7\n",
            "decay_rate 0.01\n",
            "Score over time: 6.27284\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.7\n",
            "decay_rate 0.01\n",
            "Score over time: 6.35654\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.7\n",
            "decay_rate 0.01\n",
            "Score over time: 6.42542\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.4\n",
            "decay_rate 0.1\n",
            "Score over time: 5.83244\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.4\n",
            "decay_rate 0.1\n",
            "Score over time: 6.55402\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.4\n",
            "decay_rate 0.1\n",
            "Score over time: 6.74576\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.4\n",
            "decay_rate 0.1\n",
            "Score over time: 6.83764\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.618\n",
            "decay_rate 0.1\n",
            "Score over time: 6.13106\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.618\n",
            "decay_rate 0.1\n",
            "Score over time: 6.70098\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.618\n",
            "decay_rate 0.1\n",
            "Score over time: 6.82658\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.618\n",
            "decay_rate 0.1\n",
            "Score over time: 6.78372\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.7\n",
            "decay_rate 0.1\n",
            "Score over time: 6.20866\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.7\n",
            "decay_rate 0.1\n",
            "Score over time: 6.70496\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.7\n",
            "decay_rate 0.1\n",
            "Score over time: 6.81822\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.7\n",
            "decay_rate 0.1\n",
            "Score over time: 6.84714\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.4\n",
            "decay_rate 0.3\n",
            "Score over time: 5.86644\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.4\n",
            "decay_rate 0.3\n",
            "Score over time: 6.59566\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.4\n",
            "decay_rate 0.3\n",
            "Score over time: 6.78752\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.4\n",
            "decay_rate 0.3\n",
            "Score over time: 6.8303\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.618\n",
            "decay_rate 0.3\n",
            "Score over time: 6.13788\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.618\n",
            "decay_rate 0.3\n",
            "Score over time: 6.73142\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.618\n",
            "decay_rate 0.3\n",
            "Score over time: 6.84624\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.618\n",
            "decay_rate 0.3\n",
            "Score over time: 6.86468\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.7\n",
            "decay_rate 0.3\n",
            "Score over time: 6.26692\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.7\n",
            "decay_rate 0.3\n",
            "Score over time: 6.72446\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.7\n",
            "decay_rate 0.3\n",
            "Score over time: 6.8497\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.7\n",
            "decay_rate 0.3\n",
            "Score over time: 6.87186\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.4\n",
            "decay_rate 0.6\n",
            "Score over time: 5.86358\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.4\n",
            "decay_rate 0.6\n",
            "Score over time: 6.59864\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.4\n",
            "decay_rate 0.6\n",
            "Score over time: 6.79076\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.4\n",
            "decay_rate 0.6\n",
            "Score over time: 6.84062\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.618\n",
            "decay_rate 0.6\n",
            "Score over time: 6.16552\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.618\n",
            "decay_rate 0.6\n",
            "Score over time: 6.75054\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.618\n",
            "decay_rate 0.6\n",
            "Score over time: 6.85568\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.618\n",
            "decay_rate 0.6\n",
            "Score over time: 6.88834\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.7\n",
            "decay_rate 0.6\n",
            "Score over time: 6.21338\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.7\n",
            "decay_rate 0.6\n",
            "Score over time: 6.74562\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.7\n",
            "decay_rate 0.6\n",
            "Score over time: 6.87784\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.7\n",
            "decay_rate 0.6\n",
            "Score over time: 6.89262\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.4\n",
            "decay_rate 0.8\n",
            "Score over time: 5.87336\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.4\n",
            "decay_rate 0.8\n",
            "Score over time: 6.60998\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.4\n",
            "decay_rate 0.8\n",
            "Score over time: 6.81056\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.4\n",
            "decay_rate 0.8\n",
            "Score over time: 6.837\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.618\n",
            "decay_rate 0.8\n",
            "Score over time: 6.18798\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.618\n",
            "decay_rate 0.8\n",
            "Score over time: 6.73836\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.618\n",
            "decay_rate 0.8\n",
            "Score over time: 6.83296\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.618\n",
            "decay_rate 0.8\n",
            "Score over time: 6.84332\n",
            "Learning Rate -  0.25\n",
            "Gamma  0.7\n",
            "decay_rate 0.8\n",
            "Score over time: 6.24668\n",
            "Learning Rate -  0.5\n",
            "Gamma  0.7\n",
            "decay_rate 0.8\n",
            "Score over time: 6.7245\n",
            "Learning Rate -  0.7\n",
            "Gamma  0.7\n",
            "decay_rate 0.8\n",
            "Score over time: 6.83492\n",
            "Learning Rate -  0.8\n",
            "Gamma  0.7\n",
            "decay_rate 0.8\n",
            "Score over time: 6.89738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2l0yPPWIQSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(data, columns = ['learning_rate','gamma','decay_rate','score over time']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBHH6qL7LsKl",
        "colab_type": "code",
        "outputId": "301ad382-6127-4c80-c9c6-2bc4735c2256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>gamma</th>\n",
              "      <th>decay_rate</th>\n",
              "      <th>score over time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.01</td>\n",
              "      <td>5.33492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.04594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.27336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.34510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.01</td>\n",
              "      <td>5.71432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.23970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.35056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.43066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.01</td>\n",
              "      <td>5.78414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.27284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.35654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.42542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.10</td>\n",
              "      <td>5.83244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.10</td>\n",
              "      <td>6.55402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.10</td>\n",
              "      <td>6.74576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.10</td>\n",
              "      <td>6.83764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.10</td>\n",
              "      <td>6.13106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.10</td>\n",
              "      <td>6.70098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.10</td>\n",
              "      <td>6.82658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.10</td>\n",
              "      <td>6.78372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.10</td>\n",
              "      <td>6.20866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.10</td>\n",
              "      <td>6.70496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.10</td>\n",
              "      <td>6.81822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.10</td>\n",
              "      <td>6.84714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.30</td>\n",
              "      <td>5.86644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.30</td>\n",
              "      <td>6.59566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.30</td>\n",
              "      <td>6.78752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.30</td>\n",
              "      <td>6.83030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.30</td>\n",
              "      <td>6.13788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.30</td>\n",
              "      <td>6.73142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.30</td>\n",
              "      <td>6.84624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.30</td>\n",
              "      <td>6.86468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.30</td>\n",
              "      <td>6.26692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.30</td>\n",
              "      <td>6.72446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.30</td>\n",
              "      <td>6.84970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.30</td>\n",
              "      <td>6.87186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.60</td>\n",
              "      <td>5.86358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.60</td>\n",
              "      <td>6.59864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.60</td>\n",
              "      <td>6.79076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.60</td>\n",
              "      <td>6.84062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.60</td>\n",
              "      <td>6.16552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.60</td>\n",
              "      <td>6.75054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.60</td>\n",
              "      <td>6.85568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.60</td>\n",
              "      <td>6.88834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.60</td>\n",
              "      <td>6.21338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.60</td>\n",
              "      <td>6.74562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.60</td>\n",
              "      <td>6.87784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.60</td>\n",
              "      <td>6.89262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.80</td>\n",
              "      <td>5.87336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.80</td>\n",
              "      <td>6.60998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.80</td>\n",
              "      <td>6.81056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.80</td>\n",
              "      <td>6.83700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.80</td>\n",
              "      <td>6.18798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.80</td>\n",
              "      <td>6.73836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.80</td>\n",
              "      <td>6.83296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.80</td>\n",
              "      <td>6.84332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.80</td>\n",
              "      <td>6.24668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.80</td>\n",
              "      <td>6.72450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.80</td>\n",
              "      <td>6.83492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.80</td>\n",
              "      <td>6.89738</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    learning_rate  gamma  decay_rate  score over time\n",
              "0            0.25  0.400        0.01          5.33492\n",
              "1            0.50  0.400        0.01          6.04594\n",
              "2            0.70  0.400        0.01          6.27336\n",
              "3            0.80  0.400        0.01          6.34510\n",
              "4            0.25  0.618        0.01          5.71432\n",
              "5            0.50  0.618        0.01          6.23970\n",
              "6            0.70  0.618        0.01          6.35056\n",
              "7            0.80  0.618        0.01          6.43066\n",
              "8            0.25  0.700        0.01          5.78414\n",
              "9            0.50  0.700        0.01          6.27284\n",
              "10           0.70  0.700        0.01          6.35654\n",
              "11           0.80  0.700        0.01          6.42542\n",
              "12           0.25  0.400        0.10          5.83244\n",
              "13           0.50  0.400        0.10          6.55402\n",
              "14           0.70  0.400        0.10          6.74576\n",
              "15           0.80  0.400        0.10          6.83764\n",
              "16           0.25  0.618        0.10          6.13106\n",
              "17           0.50  0.618        0.10          6.70098\n",
              "18           0.70  0.618        0.10          6.82658\n",
              "19           0.80  0.618        0.10          6.78372\n",
              "20           0.25  0.700        0.10          6.20866\n",
              "21           0.50  0.700        0.10          6.70496\n",
              "22           0.70  0.700        0.10          6.81822\n",
              "23           0.80  0.700        0.10          6.84714\n",
              "24           0.25  0.400        0.30          5.86644\n",
              "25           0.50  0.400        0.30          6.59566\n",
              "26           0.70  0.400        0.30          6.78752\n",
              "27           0.80  0.400        0.30          6.83030\n",
              "28           0.25  0.618        0.30          6.13788\n",
              "29           0.50  0.618        0.30          6.73142\n",
              "30           0.70  0.618        0.30          6.84624\n",
              "31           0.80  0.618        0.30          6.86468\n",
              "32           0.25  0.700        0.30          6.26692\n",
              "33           0.50  0.700        0.30          6.72446\n",
              "34           0.70  0.700        0.30          6.84970\n",
              "35           0.80  0.700        0.30          6.87186\n",
              "36           0.25  0.400        0.60          5.86358\n",
              "37           0.50  0.400        0.60          6.59864\n",
              "38           0.70  0.400        0.60          6.79076\n",
              "39           0.80  0.400        0.60          6.84062\n",
              "40           0.25  0.618        0.60          6.16552\n",
              "41           0.50  0.618        0.60          6.75054\n",
              "42           0.70  0.618        0.60          6.85568\n",
              "43           0.80  0.618        0.60          6.88834\n",
              "44           0.25  0.700        0.60          6.21338\n",
              "45           0.50  0.700        0.60          6.74562\n",
              "46           0.70  0.700        0.60          6.87784\n",
              "47           0.80  0.700        0.60          6.89262\n",
              "48           0.25  0.400        0.80          5.87336\n",
              "49           0.50  0.400        0.80          6.60998\n",
              "50           0.70  0.400        0.80          6.81056\n",
              "51           0.80  0.400        0.80          6.83700\n",
              "52           0.25  0.618        0.80          6.18798\n",
              "53           0.50  0.618        0.80          6.73836\n",
              "54           0.70  0.618        0.80          6.83296\n",
              "55           0.80  0.618        0.80          6.84332\n",
              "56           0.25  0.700        0.80          6.24668\n",
              "57           0.50  0.700        0.80          6.72450\n",
              "58           0.70  0.700        0.80          6.83492\n",
              "59           0.80  0.700        0.80          6.89738"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gww75IaoIyNF",
        "colab_type": "code",
        "outputId": "74836b5b-35ea-4e7d-bc8e-27dc520301c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "df.loc[df['score over time'].idxmax()]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "learning_rate      0.80000\n",
              "gamma              0.70000\n",
              "decay_rate         0.80000\n",
              "score over time    6.89738\n",
              "Name: 59, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exJEKF25K9h8",
        "colab_type": "text"
      },
      "source": [
        "##So after tunning different Hyperparameter\n",
        "\n",
        "*    Learning rate - 0.80000\n",
        "*    Gamma - 0.7 \n",
        "*    Decay Rate - 0.8\n",
        "*    Score Over Time - 6.89738\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx1DprFyMg2L",
        "colab_type": "code",
        "outputId": "f48cf66a-951c-4e6a-bcd5-06a7d50e8740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "\n",
        "ep = [90000,91000,92000,93000,94000,95000]\n",
        "\n",
        "\n",
        "\n",
        "for eps in ep:\n",
        "  rewards = []\n",
        "  qtable = np.zeros((state_size, action_size))\n",
        "  total_episodes = eps        # Total episodes\n",
        "  total_test_episodes = 100     # Total test episodes\n",
        "  max_steps = 99                # Max steps per episode\n",
        "\n",
        "  learning_rate = 0.8           # Learning rate\n",
        "  gamma = 0.7                 # Discounting rate\n",
        "\n",
        "  # Exploration parameters\n",
        "  epsilon = 1.0                 # Exploration rate\n",
        "  max_epsilon = 1.0             # Exploration probability at start\n",
        "  min_epsilon = 0.01            # Minimum exploration probability \n",
        "  decay_rate = 0.8  \n",
        "    # 2 For life or until learning is stopped\n",
        "  for episode in range(total_episodes):\n",
        "      # Reset the environment\n",
        "      state = env.reset()\n",
        "      step = 0\n",
        "      done = False\n",
        "      total_rewards = 0\n",
        "    \n",
        "      for step in range(max_steps):\n",
        "          # 3. Choose an action a in the current world state (s)\n",
        "          ## First we randomize a number\n",
        "          exp_exp_tradeoff = random.uniform(0,1)\n",
        "        \n",
        "          ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "          if exp_exp_tradeoff > epsilon:\n",
        "              action = np.argmax(qtable[state,:])\n",
        "        \n",
        "          # Else doing a random choice --> exploration\n",
        "          else:\n",
        "              action = env.action_space.sample()\n",
        "        \n",
        "          # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "          new_state, reward, done, info = env.step(action)\n",
        "\n",
        "          # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "          qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma *np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "          total_rewards += reward\n",
        "                \n",
        "          # Our new state is state\n",
        "          state = new_state\n",
        "        \n",
        "          # If done : finish episode\n",
        "          if done == True: \n",
        "              break\n",
        "    \n",
        "      # Reduce epsilon (because we need less and less exploration)\n",
        "      epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "      rewards.append(total_rewards)\n",
        "  print('Total no of Episodes - ',eps)\n",
        "  print (\"Score over time: \" +  str(sum(rewards)/total_episodes))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total no of Episodes -  90000\n",
            "Score over time: 7.044866666666667\n",
            "Total no of Episodes -  91000\n",
            "Score over time: 7.12478021978022\n",
            "Total no of Episodes -  92000\n",
            "Score over time: 7.107239130434783\n",
            "Total no of Episodes -  93000\n",
            "Score over time: 7.110204301075269\n",
            "Total no of Episodes -  94000\n",
            "Score over time: 7.110085106382979\n",
            "Total no of Episodes -  95000\n",
            "Score over time: 7.0893578947368425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0yfyhUoSqZI",
        "colab_type": "text"
      },
      "source": [
        "### The Total number of Episodes of 93000 I get the score time - 7.135827956989247"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7r5INISQ0VL",
        "colab_type": "code",
        "outputId": "798598c9-3af9-4bcb-fda0-3ddb47e8a5bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "\n",
        "st = [155,160,170,175,180,185]\n",
        "\n",
        "\n",
        "\n",
        "for stp in st:\n",
        "  rewards = []\n",
        "  qtable = np.zeros((state_size, action_size))\n",
        "  total_episodes = 93000        # Total episodes\n",
        "  total_test_episodes = 100     # Total test episodes\n",
        "  max_steps = stp               # Max steps per episode\n",
        "\n",
        "  learning_rate = 0.8           # Learning rate\n",
        "  gamma = 0.7                 # Discounting rate\n",
        "\n",
        "  # Exploration parameters\n",
        "  epsilon = 1.0                 # Exploration rate\n",
        "  max_epsilon = 1.0             # Exploration probability at start\n",
        "  min_epsilon = 0.01            # Minimum exploration probability \n",
        "  decay_rate = 0.8  \n",
        "  #learning_rate = rate\n",
        "    # 2 For life or until learning is stopped\n",
        "  for episode in range(total_episodes):\n",
        "      # Reset the environment\n",
        "      state = env.reset()\n",
        "      step = 0\n",
        "      done = False\n",
        "      total_rewards = 0\n",
        "    \n",
        "      for step in range(max_steps):\n",
        "          # 3. Choose an action a in the current world state (s)\n",
        "          ## First we randomize a number\n",
        "          exp_exp_tradeoff = random.uniform(0,1)\n",
        "        \n",
        "          ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "          if exp_exp_tradeoff > epsilon:\n",
        "              action = np.argmax(qtable[state,:])\n",
        "        \n",
        "          # Else doing a random choice --> exploration\n",
        "          else:\n",
        "              action = env.action_space.sample()\n",
        "        \n",
        "          # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "          new_state, reward, done, info = env.step(action)\n",
        "\n",
        "          # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "          qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma *np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "          total_rewards += reward\n",
        "                \n",
        "          # Our new state is state\n",
        "          state = new_state\n",
        "        \n",
        "          # If done : finish episode\n",
        "          if done == True: \n",
        "              break\n",
        "    \n",
        "      # Reduce epsilon (because we need less and less exploration)\n",
        "      epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "      rewards.append(total_rewards)\n",
        "  print('Max steps - ',stp)\n",
        "  print (\"Score over time: \" +  str(sum(rewards)/total_episodes))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max steps -  155\n",
            "Score over time: 7.101032258064516\n",
            "Max steps -  160\n",
            "Score over time: 7.1060860215053765\n",
            "Max steps -  170\n",
            "Score over time: 7.130430107526882\n",
            "Max steps -  175\n",
            "Score over time: 7.144247311827957\n",
            "Max steps -  180\n",
            "Score over time: 7.145086021505376\n",
            "Max steps -  185\n",
            "Score over time: 7.11710752688172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j24_KFxccF72",
        "colab_type": "code",
        "outputId": "b865d2d6-97b2-4e3d-abbc-5f7703dddd4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "\n",
        "min_eps = [0.00001,0.00002,0.00003,0.00006]\n",
        "\n",
        "\n",
        "\n",
        "for m_e in min_eps:\n",
        "  rewards = []\n",
        "  qtable = np.zeros((state_size, action_size))\n",
        "  total_episodes = 93000        # Total episodes\n",
        "  total_test_episodes = 100     # Total test episodes\n",
        "  max_steps = 180               # Max steps per episode\n",
        "\n",
        "  learning_rate = 0.8           # Learning rate\n",
        "  gamma = 0.7                 # Discounting rate\n",
        "\n",
        "  # Exploration parameters\n",
        "  epsilon = 1.0                 # Exploration rate\n",
        "  max_epsilon = 1.0             # Exploration probability at start\n",
        "  min_epsilon = m_e            # Minimum exploration probability \n",
        "  decay_rate = 0.8  \n",
        "  #learning_rate = rate\n",
        "    # 2 For life or until learning is stopped\n",
        "  for episode in range(total_episodes):\n",
        "      # Reset the environment\n",
        "      state = env.reset()\n",
        "      step = 0\n",
        "      done = False\n",
        "      total_rewards = 0\n",
        "    \n",
        "      for step in range(max_steps):\n",
        "          # 3. Choose an action a in the current world state (s)\n",
        "          ## First we randomize a number\n",
        "          exp_exp_tradeoff = random.uniform(0,1)\n",
        "        \n",
        "          ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "          if exp_exp_tradeoff > epsilon:\n",
        "              action = np.argmax(qtable[state,:])\n",
        "        \n",
        "          # Else doing a random choice --> exploration\n",
        "          else:\n",
        "              action = env.action_space.sample()\n",
        "        \n",
        "          # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "          new_state, reward, done, info = env.step(action)\n",
        "\n",
        "          # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "          qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma *np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "          total_rewards += reward\n",
        "                \n",
        "          # Our new state is state\n",
        "          state = new_state\n",
        "        \n",
        "          # If done : finish episode\n",
        "          if done == True: \n",
        "              break\n",
        "    \n",
        "      # Reduce epsilon (because we need less and less exploration)\n",
        "      epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "      rewards.append(total_rewards)\n",
        "  print('Min epsilon - ',m_e)\n",
        "  print (\"Score over time: \" +  str(sum(rewards)/total_episodes))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Min epsilon -  1e-05\n",
            "Score over time: 7.570516129032258\n",
            "Min epsilon -  2e-05\n",
            "Score over time: 7.598989247311828\n",
            "Min epsilon -  3e-05\n",
            "Score over time: 7.62789247311828\n",
            "Min epsilon -  6e-05\n",
            "Score over time: 7.605354838709677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UehJ30vKQYCP",
        "colab_type": "text"
      },
      "source": [
        "### The minmum epsilon value of 0.00003 I get the score time   7.62789247311828\n",
        "\n",
        "This leads to 21% increase from the baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR9hqfeOguN9",
        "colab_type": "text"
      },
      "source": [
        "### **Does Q-learning use value-based or policy-based iteration?** \n",
        "\n",
        "Q - learning uses value based iteration, it is a value-based reinforcement learning algorithm which is used to find the optimal action-selection policy using a Q function. Our goal is to maximize the value function Q. The Q table helps us to find the best action for each state. It helps to maximize the expected reward by selecting the best of all possible actions.\n",
        "Q(state, action) returns the expected future reward of that action at that state. This function can be estimated using Q-Learning, which iteratively updates Q(s,a) using the Bellman equation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SZQh_S9qTJX",
        "colab_type": "text"
      },
      "source": [
        " ### **What is meant by expected lifetime value in the Bellman equation?**\n",
        "\n",
        " ![alt text](https://miro.medium.com/max/3276/1*fpRuA-X7wGchI1I5gL19uA.png)\n",
        "\n",
        "\n",
        "In the Bellman equation, the gamma term is the discount rate, without that it the theoretical the future reward goes to the infinity. But using gamma it will be discounted and will decay and will converge to 0 at some point in time.\n",
        "\n",
        "For the coding purpose, this will be limited by the total episode number in the hyperparameter.\n",
        "\n",
        "It is the value we get not from just one step but if we continue to follow a policy until it stops.\n",
        " \n",
        "This is the expected lifetime value of the Bellman equation. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gerP9zZCjK8j",
        "colab_type": "text"
      },
      "source": [
        "# **Conclusion**\n",
        "![alt text](https://essay-lib.com/wp-content/uploads/2018/02/Conclusion-for-a-Research-Paper-1024x400.png)\n",
        "\n",
        "After changing policy I can conclude that argmax is the policy with the best policy with good score time. The baseline score time was 6.3 and with tunning Hyperparameter, I was able to improve it to 7.627 which is a **21% increase from the baseline**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmWmWyXaxnl7",
        "colab_type": "text"
      },
      "source": [
        "# **Citation**\n",
        "\n",
        "1. I have taken baesline model fromhttps://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/Taxi-v2/Q%20Learning%20with%20OpenAI%20Taxi-v2%20video%20version.ipynbhttps://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/Taxi-v2/Q%20Learning%20with%20OpenAI%20Taxi-v2%20video%20version.ipynb\n",
        "as given by professor\n",
        "\n",
        "2. All other code are written only by me."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0nk5-rP6e1E",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Copyright 2020 Abhishek Garga Maheshwarappa\n",
        "\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    }
  ]
}